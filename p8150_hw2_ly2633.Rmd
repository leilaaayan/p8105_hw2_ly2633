---
title: "p8105_hw2_ly2633"
author: "Leila Yan"
date: "2024-09-25"
output: github_document
---

```{r setup}
library(tidyverse)
```
## Problem 1
Read in the dataset
```{r}
transit_df = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(line:entry, vending, ada) %>%
  mutate(entry = case_match(entry, 
                            "YES" ~ TRUE,
                            "NO" ~ FALSE))
```
Write a short paragraph about this dataset â€“ explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy? 

Answers:
The dataset contains information about the entrances and exits of subway stations in New York City. It includes several key variables: line, which identifies the subway line associated with each entrance or exit, and station_name, which provides the name of the station. The geographical coordinates of each station are given by station_latitude and station_longitude. The dataset also has columns for route1 through route11, which indicate the subway routes available at each station. entrance_type describes the type of entrance, such as stair, elevator, door, escalator, etc, while entry indicates whether entry is allowed at the specified location (TRUE/FALSE). The vending column shows whether there are vending machines available for purchasing tickets, and ada is a boolean variable indicating whether the entrance or exit is compliant with the Americans with Disabilities Act (ADA). This dataset contains 1868 entries and 32 columns (1868 x 32). Yes, the data is tidy after I performed the data cleaning steps, such as clean_names(). 

The data cleaning process began by loading the dataset from a CSV file. Next, janitor::clean_names() was used to standardize column names, converting them to lowercase and snake_case for easier reference. After that, specific columns were selected, including those from line through entry, as well as vending and ada, which were deemed most relevant for the analysis. Finally, the entry column was modified using case_match() to convert the original values of "YES" and "NO" into logical values (TRUE and FALSE), making the data easier to work with.


How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here. How many stations are ADA compliant? What proportion of station entrances / exits without vending allow entrance?
```{r}
#distinct stations
distinct_stations = 
  transit_df %>%
  distinct(line, station_name) %>%
  nrow()

distinct_stations

#ada compliance
ada_compliant_stations =
  transit_df %>%
  filter(ada == TRUE) %>%
  distinct(line, station_name) %>%
  nrow()

ada_compliant_stations

#proportion of station/entrances/exits without vending allow entrance
without_vending_entry = 
  transit_df %>%
  filter(vending == "NO", entry == TRUE) %>%
  nrow()

total_without_vending =
  transit_df %>%
  filter(vending == "NO") %>%
  nrow()

proportion_without_vending_entry <- without_vending_entry / total_without_vending

proportion_without_vending_entry
```
Answers:
The dataset contains a total of 465 distinct subway stations in New York City, where stations are identified by both name and line. Out of these stations, 84 are compliant with ADA. Additionally, around 37.7% of station entrances or exits that do not have vending machines still allow entry. These metrics provide insights into the accessibility and functionality of the NYC subway system.


Reformat data so that route number and route name are distinct variables. 
```{r}
transit_df_long=
  transit_df %>%
  mutate(across(starts_with("route"), as.character)) %>%
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name", 
    values_drop_na = TRUE
    )
```

How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?
```{r}
stations_A_train =
  transit_df_long %>%
  filter(route_name == "A")


distinct_stations_A_train =
  stations_A_train %>%
  distinct(station_name) %>%
  nrow()

distinct_stations_A_train

ada_compliant_stations_A_train =
  stations_A_train %>%
  filter(ada == TRUE) %>%   # Filter for ADA compliant stations
  distinct(station_name) %>%  # Get distinct station names
  nrow()

ada_compliant_stations_A_train
```
Answers:
56 distinct stations serve the A train. Of the stations that serve the A train, 16 stations are ADA compliant.



## Problem 2

Read and clean the Mr. Trash Wheel sheet:
- specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
library(readxl)
mr_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>%  # Omit rows where the 'dumpster' column is NA
 # select(where(~ !all(is.na(.)))) %>% # omit non-data entries
  mutate(sports_balls = as.integer(round(sports_balls)))



mr_trash_wheel = read_excel("Data/202409 Trash Wheel Collection Data.xlsx", range="A2:N587") %>% janitor:: clean_names() %>% mutate(sports_balls = round(as.numeric(sports_balls),0)) %>% filter(!is.na(dumpster))

```
585 observations 14 columns

Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.

```{r}
prof_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster))
# 119 entries, 13 columns

gwynnda_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster))
# 263 entries, 12 columns

# convert year column to character for consistency
gwynnda_trash_wheel_df <- gwynnda_trash_wheel_df %>%
  mutate(year = as.character(year))

prof_trash_wheel_df <- prof_trash_wheel_df %>%
  mutate(year = as.character(year))

mr_trash_wheel_df <- mr_trash_wheel_df %>%
  mutate(year = as.character(year))

# Now combine the datasets
combined_trash_wheel_df <- bind_rows(
  gwynnda_trash_wheel_df,
  prof_trash_wheel_df,
  mr_trash_wheel_df
)

```

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in June of 2022?

```{r setup, include=FALSE}
#total weight of trash collected by Professor Trash Wheel
total_weight =
  prof_trash_wheel_df %>%
  summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))

print(total_weight)

total_cigarette_butts =
  gwynnda_trash_wheel_df %>%
  filter(month == "June", year == 2022) %>%  # Filter for June 2022
  summarise(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))  # Sum the cigarette butts

print(total_cigarette_butts)
```
The total weight of trash collected by Professor Trash Wheel is 246.74 tons. The total number of cigarette butts collected by Gwynnda in June of 2022 is 18120. The combined dataset, combined_trash_wheel_df, contains 845 observations from three different trash wheel datasets (gwynnda_trash_wheel_df, prof_trash_wheel_df, and mr_trash_wheel_df). Each observation represents a collection event over various dates, capturing data such as the number of plastic bottles, polystyrene, and cigarette butts collected, as well as the weight and volume of trash in each dumpster. Key variables include dumpster, which identifies collection events, and temporal variables like month, year, and date. Waste metrics include weight_tons and volume_cubic_yards, providing measures of the collected trash volume. The dataset also tracks specific types of waste, such as plastic_bottles, polystyrene, and cigarette_butts. Additionally, some columns, such as glass_bottles, homes_powered, and sports_balls, contain NA values, indicating that these measurements were not consistently recorded across all trash wheels. This comprehensive dataset is valuable for analyzing temporal and spatial trends in waste collection, understanding the environmental impact, and informing waste management strategies. Insights derived from this data can support targeted interventions to reduce specific types of waste, like plastic or cigarette butts, and provide a broader understanding of changes in waste generation patterns over time.


## Problem 3
Read in datasets, clean, and tidy
```{r}
# remove bakers' last name and convert baker_name to baker
bakers_df = 
   read_csv("gbb_datasets/bakers.csv") %>%
  janitor::clean_names() %>%
  mutate(baker = sub(" .*", "", baker_name)) %>% 
  select(-baker_name)

bakes_df = 
   read_csv("gbb_datasets/bakes.csv") %>%
  janitor::clean_names() %>%
  drop_na()

results_df = 
   read_csv("gbb_datasets/results.csv", skip=2) %>%
  janitor::clean_names() %>%
  drop_na()
```

check for completeness
```{r}
anti_join(bakes_df, bakers_df, results_df, by = "baker")
```

Merge datasets into one
```{r}
mergeingtwo_df = 
  left_join(bakers_df, bakes_df, by = "baker")

bakingmerged_df = 
  left_join(mergeingtwo_df, results_df, by = "baker") %>%
  arrange(series) %>%
  select(-series.x, -series.y, -episode.y) %>%
  relocate(series)


# Display the cleaned merged dataset
print(bakingmerged_df)
```
Export the result as a CSV in the directory containing the original datasets.
```{r}
write_csv(bakingmerged_df, "gbb_datasets/bakingmerged_df.csv")
```

Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

Answers:
In the data cleaning process, I began by standardizing column names across datasets using janitor::clean_names() for consistency. In the bakers_df, I simplified the baker identification by extracting only the first name from the baker_name column using mutate, and removed the original baker_name column with select(-baker_name). This choice was made to avoid complexity when joining datasets, although it raises the question of whether using only the first name might lead to ambiguity if there are bakers with the same first name. The bakes_df and results_df were also read in and cleaned similarly, with results_df requiring skipping the first two rows to properly align the data structure.

I checked for completeness by using anti_join() to identify bakers present in bakes_df, results_df, and bakers_df, ensuring that no key records were missing for merging. For merging, I used left_join() twice: first to combine bakers_df and bakes_df into mergeingtwo_df, then to join results_df with mergeingtwo_df to form bakingmerged_df. I chose left_join() to retain all bakers, even if they didnâ€™t have corresponding entries in the bakes or results datasets, which is important for maintaining the full list of participants. Finally, I arranged the data by series and relocated this column to the beginning for easier readability and analysis. In the final merged dataset, I removed the extra two series columns and the extra episode column to clean and tidy the dataset. The final dataset is structured to provide comprehensive information on each baker, including their details, bakes, and results, facilitating subsequent analysis of performance across different series.


Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table â€“ were there any predictable overall winners? Any surprises?

```{r}
library(knitr)

star_bakers =
  bakingmerged_df %>%
  filter(result == "STAR BAKER" | result == "WINNER") %>%
  select(series, episode.x, baker, result) %>%
  filter(series >= 5 & series <= 10) %>% 
  distinct(series, episode.x, baker, .keep_all = TRUE) %>%
  arrange(series, episode.x) %>%
  kable(col.names = c("series", "episode.x", "baker", "result"))
print(star_bakers)

write_csv(star_bakers, "star_bakers_summary.csv")
```

Answers:
For instance, bakers like Kate, who consistently won the "STAR BAKER" title across multiple episodes, appeared to be strong contenders for the overall win, indicating predictability in their high performance throughout the series. However, there were also surprising elements, such as Nancy winning the overall "WINNER" title despite fewer "STAR BAKER" titles compared to others like Kate or Richard, suggesting that Nancy's performance peaked at crucial moments. Such variability highlights both predictable trends in consistent bakers and unexpected wins by those who may have outperformed when it mattered most.



Import, clean, tidy, and organize the viewership data in viewership.csv. Show the first 10 rows of this dataset. 
```{r}
viewers_df = 
   read_csv("gbb_datasets/viewers.csv") %>%
  janitor::clean_names() 

head(viewers_df, 10)
```


What was the average viewership in Season 1? In Season 5?
```{r}
viewers_df %>%
  summarize(average_viewership_1 = mean(series_1, na.rm = TRUE))

viewers_df %>%
  summarize(average_viewership_5 = mean(series_5, na.rm = TRUE))
```
Answers:
The average viewership in season 1 is 2.77 and that in season 5 is 10.0393.
