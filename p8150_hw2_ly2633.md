p8105_hw2_ly2633
================
Leila Yan
2024-09-25

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

## Problem 1

Read in the dataset

``` r
transit_df = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(line:entry, vending, ada) %>%
  mutate(entry = case_match(entry, 
                            "YES" ~ TRUE,
                            "NO" ~ FALSE))
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Write a short paragraph about this dataset – explain briefly what
variables the dataset contains, describe your data cleaning steps so
far, and give the dimension (rows x columns) of the resulting dataset.
Are these data tidy?

Answers: The dataset contains information about the entrances and exits
of subway stations in New York City. It includes several key variables:
line, which identifies the subway line associated with each entrance or
exit, and station_name, which provides the name of the station. The
geographical coordinates of each station are given by station_latitude
and station_longitude. The dataset also has columns for route1 through
route11, which indicate the subway routes available at each station.
entrance_type describes the type of entrance, such as stair, elevator,
door, escalator, etc, while entry indicates whether entry is allowed at
the specified location (TRUE/FALSE). The vending column shows whether
there are vending machines available for purchasing tickets, and ada is
a boolean variable indicating whether the entrance or exit is compliant
with the Americans with Disabilities Act (ADA). This dataset contains
1868 entries and 32 columns (1868 x 32). Yes, the data is tidy after I
performed the data cleaning steps, such as clean_names().

The data cleaning process began by loading the dataset from a CSV file.
Next, janitor::clean_names() was used to standardize column names,
converting them to lowercase and snake_case for easier reference. After
that, specific columns were selected, including those from line through
entry, as well as vending and ada, which were deemed most relevant for
the analysis. Finally, the entry column was modified using case_match()
to convert the original values of “YES” and “NO” into logical values
(TRUE and FALSE), making the data easier to work with.

How many distinct stations are there? Note that stations are identified
both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway;
125st Lenox); the distinct function may be useful here. How many
stations are ADA compliant? What proportion of station entrances / exits
without vending allow entrance?

``` r
#distinct stations
distinct_stations = 
  transit_df %>%
  distinct(line, station_name) %>%
  nrow()

distinct_stations
```

    ## [1] 465

``` r
#ada compliance
ada_compliant_stations =
  transit_df %>%
  filter(ada == TRUE) %>%
  distinct(line, station_name) %>%
  nrow()

ada_compliant_stations
```

    ## [1] 84

``` r
#proportion of station/entrances/exits without vending allow entrance
without_vending_entry = 
  transit_df %>%
  filter(vending == "NO", entry == TRUE) %>%
  nrow()

total_without_vending =
  transit_df %>%
  filter(vending == "NO") %>%
  nrow()

proportion_without_vending_entry <- without_vending_entry / total_without_vending

proportion_without_vending_entry
```

    ## [1] 0.3770492

Answers: The dataset contains a total of 465 distinct subway stations in
New York City, where stations are identified by both name and line. Out
of these stations, 84 are compliant with ADA. Additionally, around 37.7%
of station entrances or exits that do not have vending machines still
allow entry. These metrics provide insights into the accessibility and
functionality of the NYC subway system.

Reformat data so that route number and route name are distinct
variables.

``` r
transit_df_long=
  transit_df %>%
  mutate(across(starts_with("route"), as.character)) %>%
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name", 
    values_drop_na = TRUE
    )
```

How many distinct stations serve the A train? Of the stations that serve
the A train, how many are ADA compliant?

``` r
stations_A_train =
  transit_df_long %>%
  filter(route_name == "A")


distinct_stations_A_train =
  stations_A_train %>%
  distinct(station_name) %>%
  nrow()

distinct_stations_A_train
```

    ## [1] 56

``` r
ada_compliant_stations_A_train =
  stations_A_train %>%
  filter(ada == TRUE) %>%   # Filter for ADA compliant stations
  distinct(station_name) %>%  # Get distinct station names
  nrow()

ada_compliant_stations_A_train
```

    ## [1] 16

Answers: 56 distinct stations serve the A train. Of the stations that
serve the A train, 16 stations are ADA compliant.

## Problem 2

Read and clean the Mr. Trash Wheel sheet: - specify the sheet in the
Excel file and to omit non-data entries (rows with notes / figures;
columns containing notes) using arguments in read_excel - use reasonable
variable names - omit rows that do not include dumpster-specific data -
round the number of sports balls to the nearest integer and converts the
result to an integer variable (using as.integer)

``` r
library(readxl)
mr_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>%  # Omit rows where the 'dumpster' column is NA
 # select(where(~ !all(is.na(.)))) %>% # omit non-data entries
  mutate(sports_balls = as.integer(round(sports_balls)))
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
mr_trash_wheel = read_excel("Data/202409 Trash Wheel Collection Data.xlsx", range="A2:N587") %>% janitor:: clean_names() %>% mutate(sports_balls = round(as.numeric(sports_balls),0)) %>% filter(!is.na(dumpster))
```

585 observations 14 columns

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to both
datasets before combining.

``` r
prof_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster))
# 119 entries, 13 columns

gwynnda_trash_wheel_df =
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster))
# 263 entries, 12 columns

# convert year column to character for consistency
gwynnda_trash_wheel_df <- gwynnda_trash_wheel_df %>%
  mutate(year = as.character(year))

prof_trash_wheel_df <- prof_trash_wheel_df %>%
  mutate(year = as.character(year))

mr_trash_wheel_df <- mr_trash_wheel_df %>%
  mutate(year = as.character(year))

# Now combine the datasets
combined_trash_wheel_df <- bind_rows(
  gwynnda_trash_wheel_df,
  prof_trash_wheel_df,
  mr_trash_wheel_df
)
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in June of 2022?

``` r
#total weight of trash collected by Professor Trash Wheel
total_weight =
  prof_trash_wheel_df %>%
  summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))

print(total_weight)
```

    ## # A tibble: 1 × 1
    ##   total_weight_tons
    ##               <dbl>
    ## 1              247.

``` r
total_cigarette_butts =
  gwynnda_trash_wheel_df %>%
  filter(month == "June", year == 2022) %>%  # Filter for June 2022
  summarise(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))  # Sum the cigarette butts
print(total_cigarette_butts)
```

    ## # A tibble: 1 × 1
    ##   total_cigarette_butts
    ##                   <dbl>
    ## 1                 18120

The total weight of trash collected by Professor Trash Wheel is 246.74
tons. The total number of cigarette butts collected by Gwynnda in June
of 2022 is 1.812^{4}. The combined dataset, combined_trash_wheel_df,
contains 1033 observations from three different trash wheel datasets
(gwynnda_trash_wheel_df, prof_trash_wheel_df, and mr_trash_wheel_df).
Each observation represents a collection event over various dates,
capturing data such as the number of plastic bottles, polystyrene, and
cigarette butts collected, as well as the weight and volume of trash in
each dumpster. Key variables include dumpster, which identifies
collection events, and temporal variables like month, year, and date.
Waste metrics include weight_tons and volume_cubic_yards, providing
measures of the collected trash volume. The dataset also tracks specific
types of waste, such as plastic_bottles, polystyrene, and
cigarette_butts. Additionally, some columns, such as glass_bottles,
homes_powered, and sports_balls, contain NA values, indicating that
these measurements were not consistently recorded across all trash
wheels. This comprehensive dataset is valuable for analyzing temporal
and spatial trends in waste collection, understanding the environmental
impact, and informing waste management strategies. Insights derived from
this data can support targeted interventions to reduce specific types of
waste, like plastic or cigarette butts, and provide a broader
understanding of changes in waste generation patterns over time.

## Problem 3

Read in datasets, clean, and tidy

``` r
# remove bakers' last name and convert baker_name to baker
bakers_df = 
  read_csv("gbb_datasets/bakers.csv") %>%
  janitor::clean_names() %>%
  mutate(baker = sub(" .*", "", baker_name)) %>% 
  rename(series_bakers = series) %>%
  select(-baker_name)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = 
   read_csv("gbb_datasets/bakes.csv") %>%
  janitor::clean_names() %>%
  rename(series_bakes = series, episode_bakes = episode) %>%
  drop_na()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = 
   read_csv("gbb_datasets/results.csv", skip=2) %>%
  janitor::clean_names() %>%
  rename(series_results = series, episode_results = episode) %>%
  drop_na()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

check for completeness

``` r
anti_join(bakes_df, bakers_df, results_df, by = "baker")
```

    ## # A tibble: 8 × 5
    ##   series_bakes episode_bakes baker    signature_bake                show_stopper
    ##          <dbl>         <dbl> <chr>    <chr>                         <chr>       
    ## 1            2             1 "\"Jo\"" Chocolate Orange CupcakesOra… Chocolate a…
    ## 2            2             2 "\"Jo\"" Caramelised Onion, Gruyere a… Raspberry a…
    ## 3            2             3 "\"Jo\"" Stromboli flavored with Mozz… Unknown     
    ## 4            2             4 "\"Jo\"" Lavender Biscuits             Blueberry M…
    ## 5            2             5 "\"Jo\"" Salmon and Asparagus Pie      Apple and R…
    ## 6            2             6 "\"Jo\"" Rum and Raisin Baked Cheesec… Limoncello …
    ## 7            2             7 "\"Jo\"" Raspberry & Strawberry Mouss… Pain Aux Ra…
    ## 8            2             8 "\"Jo\"" Raspberry and Blueberry Mill… Mini Victor…

Merge datasets into one

``` r
mergeingtwo_df = 
  left_join(bakers_df, bakes_df, by = "baker")
```

    ## Warning in left_join(bakers_df, bakes_df, by = "baker"): Detected an unexpected many-to-many relationship between `x` and `y`.
    ## ℹ Row 1 of `x` matches multiple rows in `y`.
    ## ℹ Row 2 of `y` matches multiple rows in `x`.
    ## ℹ If a many-to-many relationship is expected, set `relationship =
    ##   "many-to-many"` to silence this warning.

``` r
bakingmerged_df = 
  left_join(mergeingtwo_df, results_df, by = "baker") %>%
  relocate(baker)
```

    ## Warning in left_join(mergeingtwo_df, results_df, by = "baker"): Detected an unexpected many-to-many relationship between `x` and `y`.
    ## ℹ Row 1 of `x` matches multiple rows in `y`.
    ## ℹ Row 166 of `y` matches multiple rows in `x`.
    ## ℹ If a many-to-many relationship is expected, set `relationship =
    ##   "many-to-many"` to silence this warning.

``` r
bakingmerged_df = bakingmerged_df %>% pivot_longer(
                          cols=c("series_bakers", "series_bakes", "series_results"),
                          names_to = "orig_series_names",
                          values_to= "series",
                          values_drop_na = TRUE
                    )

bakingmerged_df = bakingmerged_df %>% pivot_longer(
                          cols=c("episode_bakes", "episode_results"),
                          names_to = "orig_episode_names",
                          values_to= "episodes",
                          values_drop_na = TRUE
                    )
bakingmerged_df = 
   bakingmerged_df %>% select(-orig_series_names, -orig_episode_names) %>% arrange()


# Display the cleaned merged dataset
print(bakingmerged_df)
```

    ## # A tibble: 39,108 × 10
    ##    baker baker_age baker_occupation hometown         signature_bake show_stopper
    ##    <chr>     <dbl> <chr>            <chr>            <chr>          <chr>       
    ##  1 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  2 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  3 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  4 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  5 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  6 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  7 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  8 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ##  9 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ## 10 Ali          25 Charity worker   Saltley, Birmin… Rose and Pist… Chocolate, …
    ## # ℹ 39,098 more rows
    ## # ℹ 4 more variables: technical <dbl>, result <chr>, series <dbl>,
    ## #   episodes <dbl>

Export the result as a CSV in the directory containing the original
datasets.

``` r
write_csv(bakingmerged_df, "gbb_datasets/bakingmerged_df.csv")
```

Describe your data cleaning process, including any questions you have or
choices you made. Briefly discuss the final dataset.

Answers: In the data cleaning process, I began by standardizing column
names across datasets using janitor::clean_names() for consistency. In
the bakers_df, I simplified the baker identification by extracting only
the first name from the baker_name column using mutate, and removed the
original baker_name column with select(-baker_name). This choice was
made to avoid complexity when joining datasets, although it raises the
question of whether using only the first name might lead to ambiguity if
there are bakers with the same first name. The bakes_df and results_df
were also read in and cleaned similarly, with results_df requiring
skipping the first two rows to properly align the data structure. For
each dataset, I also renamed the series and episodes columns with
“series_datasetname” and “episode_datasetname”. I added an underscore of
the name of the dataset to avoid confusion in the merging step.

I then checked for completeness by using anti_join() to identify bakers
present in bakes_df, results_df, and bakers_df, ensuring that no key
records were missing for merging. For merging, I used left_join() twice:
first to combine bakers_df and bakes_df into mergeingtwo_df, then to
join results_df with mergeingtwo_df to form bakingmerged_df. I chose
left_join() to retain all bakers, even if they didn’t have corresponding
entries in the bakes or results datasets, which is important for
maintaining the full list of participants. I then performed pivot_longer
so that all episodes are transposed into one column and all series are
transposed into one column to ensure that my final merged dataset only
contains one series column and one episode column. The final dataset is
structured to provide comprehensive information on each baker, including
their details, bakes, and results, facilitating subsequent analysis of
performance across different series.

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises?

``` r
library(knitr)

star_bakers =
  bakingmerged_df %>%
  filter(result == "STAR BAKER" | result == "WINNER") %>%
  select(series, episodes, baker, result) %>%
  filter(series >= 5 & series <= 10) %>% 
  distinct(series, episodes, baker, .keep_all = TRUE) %>%
  arrange(series, episodes)

print(star_bakers)
```

    ## # A tibble: 251 × 4
    ##    series episodes baker   result    
    ##     <dbl>    <dbl> <chr>   <chr>     
    ##  1      5        1 Chetna  STAR BAKER
    ##  2      5        1 Kate    STAR BAKER
    ##  3      5        1 Luis    STAR BAKER
    ##  4      5        1 Nancy   STAR BAKER
    ##  5      5        1 Richard STAR BAKER
    ##  6      5        2 Chetna  STAR BAKER
    ##  7      5        2 Kate    STAR BAKER
    ##  8      5        2 Luis    STAR BAKER
    ##  9      5        2 Nancy   STAR BAKER
    ## 10      5        2 Richard STAR BAKER
    ## # ℹ 241 more rows

Answers: For instance, bakers like Kate, who consistently won the “STAR
BAKER” title across multiple episodes, appeared to be strong contenders
for the overall win, indicating predictability in their high performance
throughout the series. However, there were also surprising elements,
such as Nancy winning the overall “WINNER” title despite fewer “STAR
BAKER” titles compared to others like Kate or Richard, suggesting that
Nancy’s performance peaked at crucial moments. Such variability
highlights both predictable trends in consistent bakers and unexpected
wins by those who may have outperformed when it mattered most.

Import, clean, tidy, and organize the viewership data in viewership.csv.
Show the first 10 rows of this dataset.

``` r
viewers_df = 
   read_csv("gbb_datasets/viewers.csv") %>%
  janitor::clean_names() 
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

What was the average viewership in Season 1? In Season 5?

``` r
viewers_df %>%
  summarize(average_viewership_1 = mean(series_1, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##   average_viewership_1
    ##                  <dbl>
    ## 1                 2.77

``` r
viewers_df %>%
  summarize(average_viewership_5 = mean(series_5, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##   average_viewership_5
    ##                  <dbl>
    ## 1                 10.0

Answers: The average viewership in season 1 is 2.77 and that in season 5
is 10.0393.
